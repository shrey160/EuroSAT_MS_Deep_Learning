{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0+cpu'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import random\n",
    "import rasterio\n",
    "from PIL import Image\n",
    "\n",
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# Note: this notebook requires torch >= 1.10.0\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup device-agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup path to data folder\n",
    "data_path = Path(\"../data/\")\n",
    "image_path = data_path / \"Euro_MS_Test_red\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def walk_through_dir(dir_path):\n",
    "  \"\"\"\n",
    "  Walks through dir_path returning its contents.\n",
    "  Args:\n",
    "    dir_path (str or pathlib.Path): target directory\n",
    "  \n",
    "  Returns:\n",
    "    A print out of:\n",
    "      number of subdiretories in dir_path\n",
    "      number of images (files) in each subdirectory\n",
    "      name of each subdirectory\n",
    "  \"\"\"\n",
    "  for dirpath, dirnames, filenames in os.walk(dir_path):\n",
    "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 directories and 0 images in '..\\data\\Euro_MS_Test_red'.\n",
      "There are 10 directories and 0 images in '..\\data\\Euro_MS_Test_red\\test'.\n",
      "There are 0 directories and 6 images in '..\\data\\Euro_MS_Test_red\\test\\AnnualCrop'.\n",
      "There are 0 directories and 6 images in '..\\data\\Euro_MS_Test_red\\test\\Forest'.\n",
      "There are 0 directories and 6 images in '..\\data\\Euro_MS_Test_red\\test\\HerbaceousVegetation'.\n",
      "There are 0 directories and 5 images in '..\\data\\Euro_MS_Test_red\\test\\Highway'.\n",
      "There are 0 directories and 5 images in '..\\data\\Euro_MS_Test_red\\test\\Industrial'.\n",
      "There are 0 directories and 4 images in '..\\data\\Euro_MS_Test_red\\test\\Pasture'.\n",
      "There are 0 directories and 5 images in '..\\data\\Euro_MS_Test_red\\test\\PermanentCrop'.\n",
      "There are 0 directories and 6 images in '..\\data\\Euro_MS_Test_red\\test\\Residential'.\n",
      "There are 0 directories and 5 images in '..\\data\\Euro_MS_Test_red\\test\\River'.\n",
      "There are 0 directories and 6 images in '..\\data\\Euro_MS_Test_red\\test\\SeaLake'.\n",
      "There are 10 directories and 0 images in '..\\data\\Euro_MS_Test_red\\train'.\n",
      "There are 0 directories and 24 images in '..\\data\\Euro_MS_Test_red\\train\\AnnualCrop'.\n",
      "There are 0 directories and 24 images in '..\\data\\Euro_MS_Test_red\\train\\Forest'.\n",
      "There are 0 directories and 24 images in '..\\data\\Euro_MS_Test_red\\train\\HerbaceousVegetation'.\n",
      "There are 0 directories and 20 images in '..\\data\\Euro_MS_Test_red\\train\\Highway'.\n",
      "There are 0 directories and 20 images in '..\\data\\Euro_MS_Test_red\\train\\Industrial'.\n",
      "There are 0 directories and 16 images in '..\\data\\Euro_MS_Test_red\\train\\Pasture'.\n",
      "There are 0 directories and 20 images in '..\\data\\Euro_MS_Test_red\\train\\PermanentCrop'.\n",
      "There are 0 directories and 24 images in '..\\data\\Euro_MS_Test_red\\train\\Residential'.\n",
      "There are 0 directories and 20 images in '..\\data\\Euro_MS_Test_red\\train\\River'.\n",
      "There are 0 directories and 24 images in '..\\data\\Euro_MS_Test_red\\train\\SeaLake'.\n"
     ]
    }
   ],
   "source": [
    "walk_through_dir(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('../data/Euro_MS_Test_red/train'),\n",
       " WindowsPath('../data/Euro_MS_Test_red/test'))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup train and testing paths\n",
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\"\n",
    "\n",
    "train_dir, test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write transform for image\n",
    "data_transform = transforms.Compose([\n",
    "    # Turn the image into a torch.Tensor\n",
    "    transforms.ToTensor() # this also converts all pixel values from 0 to 255 to be between 0.0 and 1.0 \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageFolder_Size(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes, self.class_to_idx = self._find_classes()\n",
    "        self.samples = self._make_dataset()\n",
    "\n",
    "    def _find_classes(self):\n",
    "        classes = [d.name for d in os.scandir(self.root_dir) if d.is_dir()]\n",
    "        classes.sort()\n",
    "        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "        return classes, class_to_idx\n",
    "\n",
    "    def _make_dataset(self):\n",
    "        images = []\n",
    "        for target_class in sorted(self.class_to_idx.keys()):\n",
    "            class_index = self.class_to_idx[target_class]\n",
    "            target_dir = os.path.join(self.root_dir, target_class)\n",
    "            for root, _, fnames in sorted(os.walk(target_dir, followlinks=True)):     \n",
    "                for fname in sorted(fnames):\n",
    "                    path = os.path.join(root, fname)\n",
    "                    item = (path, class_index)\n",
    "                    images.append(item)\n",
    "        return images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        with rasterio.open(path) as f:\n",
    "            img_dat = f.read()\n",
    "            img = np.asarray(img_dat,dtype=np.float32)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            img = img.permute(1,0,2)                      # to get [height,width,colour_channel as op]\n",
    "        return img, target\n",
    "    \n",
    "    def set_samples(self, num_samples):\n",
    "        self.samples = self.samples[:num_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:\n",
      "<__main__.CustomImageFolder_Size object at 0x0000018381C37970>\n",
      "Test data:\n",
      "<__main__.CustomImageFolder_Size object at 0x0000018381C37BB0>\n"
     ]
    }
   ],
   "source": [
    "# Use ImageFolder to create dataset(s)\n",
    "\n",
    "# presently testing out a data set for customized size\n",
    "\n",
    "train_data = CustomImageFolder_Size(root_dir=train_dir, # target folder of images\n",
    "                                  transform=data_transform,\n",
    "                                  ) # transforms to perform on data (images)\n",
    "                        \n",
    "\n",
    "test_data = CustomImageFolder_Size(root_dir=test_dir, \n",
    "                                 transform=data_transform,)\n",
    "\n",
    "print(f\"Train data:\\n{train_data}\\nTest data:\\n{test_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AnnualCrop',\n",
       " 'Forest',\n",
       " 'HerbaceousVegetation',\n",
       " 'Highway',\n",
       " 'Industrial',\n",
       " 'Pasture',\n",
       " 'PermanentCrop',\n",
       " 'Residential',\n",
       " 'River',\n",
       " 'SeaLake']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = train_data.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AnnualCrop': 0,\n",
       " 'Forest': 1,\n",
       " 'HerbaceousVegetation': 2,\n",
       " 'Highway': 3,\n",
       " 'Industrial': 4,\n",
       " 'Pasture': 5,\n",
       " 'PermanentCrop': 6,\n",
       " 'Residential': 7,\n",
       " 'River': 8,\n",
       " 'SeaLake': 9}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can also get class names as a dict\n",
    "class_dict = train_data.class_to_idx\n",
    "class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x18381a503d0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x18381a50430>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn train and test Datasets into DataLoaders\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(dataset=train_data, \n",
    "                              batch_size=1, # how many samples per batch?\n",
    "                              #num_workers=1, # how many subprocesses to use for data loading? (higher = more)\n",
    "                              shuffle=True) # shuffle the data?\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data, \n",
    "                             batch_size=1, \n",
    "                             #num_workers=1, \n",
    "                             shuffle=False) # don't usually need to shuffle testing data\n",
    "\n",
    "train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([1, 13, 64, 64]) -> [batch_size, color_channels, height, width]\n",
      "Label shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "img, label = next(iter(train_dataloader))\n",
    "\n",
    "# Batch size will now be 1, try changing the batch_size parameter above and see what happens\n",
    "print(f\"Image shape: {img.shape} -> [batch_size, color_channels, height, width]\")\n",
    "print(f\"Label shape: {label.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyVGG(nn.Module):\n",
    "    \"\"\"\n",
    "    Model architecture copying TinyVGG from: \n",
    "    https://poloclub.github.io/cnn-explainer/\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
    "        super().__init__()\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape, \n",
    "                      out_channels=hidden_units, \n",
    "                      kernel_size=3, # how big is the square that's going over the image?\n",
    "                      stride=1, # default\n",
    "                      padding=1), # options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units, \n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                         stride=2) # default stride value is same as kernel_size\n",
    "        )\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # Where did this in_features shape come from? \n",
    "            # It's because each layer of our network compresses and changes the shape of our inputs data.\n",
    "            nn.Linear(in_features=hidden_units*16*16,\n",
    "                      out_features=output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.conv_block_1(x)\n",
    "        # print(x.shape)\n",
    "        x = self.conv_block_2(x)\n",
    "        # print(x.shape)\n",
    "        x = self.classifier(x)\n",
    "        # print(x.shape)\n",
    "        return x\n",
    "        # return self.classifier(self.conv_block_2(self.conv_block_1(x))) # <- leverage the benefits of operator fusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model: torch.nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader, \n",
    "              loss_fn: torch.nn.Module):\n",
    "    # Put model in eval mode\n",
    "    model.eval() \n",
    "    \n",
    "    # Setup test loss and test accuracy values\n",
    "    test_loss, test_acc = 0, 0\n",
    "    \n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # Send data to target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "    \n",
    "            # 1. Forward pass\n",
    "            test_pred_logits = model(X)\n",
    "\n",
    "            # 2. Calculate and accumulate loss\n",
    "            loss = loss_fn(test_pred_logits, y)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Calculate and accumulate accuracy\n",
    "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "            \n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = torch.load('../models/EuroSAT_M1_H39')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "TinyVGG                                  [1, 10]                   --\n",
       "├─Sequential: 1-1                        [1, 39, 32, 32]           --\n",
       "│    └─Conv2d: 2-1                       [1, 39, 64, 64]           4,602\n",
       "│    └─ReLU: 2-2                         [1, 39, 64, 64]           --\n",
       "│    └─Conv2d: 2-3                       [1, 39, 64, 64]           13,728\n",
       "│    └─ReLU: 2-4                         [1, 39, 64, 64]           --\n",
       "│    └─MaxPool2d: 2-5                    [1, 39, 32, 32]           --\n",
       "├─Sequential: 1-2                        [1, 39, 16, 16]           --\n",
       "│    └─Conv2d: 2-6                       [1, 39, 32, 32]           13,728\n",
       "│    └─ReLU: 2-7                         [1, 39, 32, 32]           --\n",
       "│    └─Conv2d: 2-8                       [1, 39, 32, 32]           13,728\n",
       "│    └─ReLU: 2-9                         [1, 39, 32, 32]           --\n",
       "│    └─MaxPool2d: 2-10                   [1, 39, 16, 16]           --\n",
       "├─Sequential: 1-3                        [1, 10]                   --\n",
       "│    └─Flatten: 2-11                     [1, 9984]                 --\n",
       "│    └─Linear: 2-12                      [1, 10]                   99,850\n",
       "==========================================================================================\n",
       "Total params: 145,636\n",
       "Trainable params: 145,636\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 103.29\n",
       "==========================================================================================\n",
       "Input size (MB): 0.21\n",
       "Forward/backward pass size (MB): 3.19\n",
       "Params size (MB): 0.58\n",
       "Estimated Total Size (MB): 3.99\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try: \n",
    "    import torchinfo\n",
    "except:\n",
    "    !pip install torchinfo\n",
    "    import torchinfo\n",
    "    \n",
    "from torchinfo import summary\n",
    "summary(model_0, input_size=[1, 13, 64, 64]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss :  0.009372412825861695  | Test accuracy :  1.0\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "test_l, test_ac = test_step(model=model_0,\n",
    "          dataloader = test_dataloader,\n",
    "          loss_fn = loss_fn)\n",
    "\n",
    "print(\"Test loss : \",test_l,\" | Test accuracy : \",test_ac)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
